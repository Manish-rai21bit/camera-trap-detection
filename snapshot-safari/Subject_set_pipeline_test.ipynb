{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject set creation pipeline\n",
    "This notebook demonstrates the pipeline used for creating the subject set. It includes the following codes:<br>\n",
    "- get_images_from_url\n",
    "- run_inference_graph_images\n",
    "- load_image_into_numpy_array\n",
    "- run_inference_for_single_image\n",
    "\n",
    "These functions have to be arranged in modules so that it can be imported for further use. The modules with these functions are available [here](https://github.com/Manish-rai21bit/camera-trap-detection/tree/master/snapshot-safari)\n",
    "\n",
    "This work is a streamlined version of [PredictAndEvaluate](https://github.com/Manish-rai21bit/camera-trap-detection/blob/master/training_demo/PredictAndEvaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"/Users/manishrai/Desktop/UMN/Research/Zooniverse/Code/tensorflow/models/research\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "# Importing the object detection utils\n",
    "# from utils import label_map_util\n",
    "# from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manishrai/Desktop/UMN/Research/Zooniverse/Code/tensorflow/models/research/object_detection/utils/visualization_utils.py:25: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2666, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/events.py\", line 88, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\", line 160, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n"
     ]
    }
   ],
   "source": [
    "import urllib, urllib.request\n",
    "import os, ssl\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "# from run_inference_graph import load_image_into_numpy_array, run_inference_for_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The set of functions here will help in creating the subject set. \n",
    "Subject set creation for bounding box requires the following functions:\n",
    "    get_images_from_url - Download the images from the url's given into a local path\n",
    "    run_inference_graph_images on the downloaded images\"\"\"\n",
    "\n",
    "def get_images_from_url(dataset, image_name_index, url_col_index, outpath):\n",
    "    \"\"\"This function takes in a dataframe and downloads the images from the url's in the column.\n",
    "    arguments:\n",
    "        dataset - dataframe\n",
    "        image_name_index - index of the column containing the image id (capture event id)\n",
    "        url_col_index - index of the columns containing the url for the image id\n",
    "        outpath - path on the local directory where the image has to be saved\n",
    "    return: \n",
    "        Downloaded images in the path - outpath\n",
    "    Usage:\n",
    "        get_images_from_url(df3, image_name_index=0, url_col_index=6, outpath = '/Users/manishrai/Desktop/test_dir/')\"\"\"\n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "        getattr(ssl, '_create_unverified_context', None)): \n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        \n",
    "        check = []\n",
    "        \n",
    "        for i in range(dataset.shape[0]):\n",
    "            if dataset.iloc[i][image_name_index] not in check:\n",
    "                j = 0\n",
    "            if dataset.iloc[i][image_name_index] in check:\n",
    "                j += 1 \n",
    "            \n",
    "            print('Processing image: %d' % i)\n",
    "            \n",
    "            urllib.request.urlretrieve(dataset.iloc[i][url_col_index], outpath+'{0}.jpg'.format(dataset.iloc[i][image_name_index] ))\n",
    "            \n",
    "def run_inference_graph_images(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS, \\\n",
    "                               NUM_CLASSES, TEST_IMAGE_PATHS, min_threshold, \\\n",
    "                               bb_outpath, PATH_TO_BB_HASHMAP):\n",
    "    \"\"\"This function takes in a list of image local-paths and runs it through the trained graph.\n",
    "    Further, using the visualization function it draws bounding boxes on each of the images and \n",
    "    saves it in a local path.\n",
    "    Arguments:\n",
    "        PATH_TO_FROZEN_GRAPH - local path of the trained frozen graph, '/frozen_inference_graph.pb'\n",
    "        PATH_TO_LABELS - local path of the labels (a mapping from class number to class name), 'label_map_focus.pbtxt'\n",
    "        NUM_CLASSES - number of detection classes\n",
    "        TEST_IMAGE_PATHS - list of test image local paths\n",
    "        min_threshold - minimum score threshold for the bounding box to be considered\n",
    "        bb_outpath - local path where to save the images with ounding poxes, /home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/snapshot-safari/snapshot-serengeti/subject_set_upload/\n",
    "        PATH_TO_BB_HASHMAP - path where bounding box information for the subjects is saved\n",
    "        \n",
    "        \"\"\"\n",
    "    # Loading the frozen graph\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "            \n",
    "    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    \n",
    "    bb_hashmap = {}\n",
    "    for image_path in TEST_IMAGE_PATHS:\n",
    "        image = Image.open(image_path)\n",
    "        if (len(np.array(image).shape) == 3):\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Considering the default dpi of matplotlib, calculating the figure size to save\n",
    "            y0, x0, c = image_np.shape\n",
    "            h = y0/72 # the default for dpi for matplotlib is 72\n",
    "            w = x0/72 # the default for dpi for matplotlib is 72\n",
    "            IMAGE_SIZE = (w, h)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "              image_np,\n",
    "              output_dict['detection_boxes'],\n",
    "              output_dict['detection_classes'],\n",
    "              output_dict['detection_scores'],\n",
    "              category_index,\n",
    "              instance_masks=output_dict.get('detection_masks'),\n",
    "              use_normalized_coordinates=True,\n",
    "              line_thickness=8,\n",
    "              min_score_thresh=min_threshold,\n",
    "              skip_labels=True,\n",
    "              skip_scores=True,\n",
    "              agnostic_mode=True\n",
    "            )\n",
    "            # make a figure without the frame\n",
    "            fig = plt.figure(frameon=False, figsize=IMAGE_SIZE)\n",
    "            # make the content fill the whole figure\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            # draw your image\n",
    "            ax.imshow(image_np)\n",
    "            # saving image with boxes on the disk\n",
    "            plt.savefig(os.path.join(bb_outpath, '{0}'.format(image_path[-14:])))\n",
    "            plt.gcf().clear()\n",
    "            bb_hashmap[image_path[-14:]] = {\n",
    "              'detection_boxes' : output_dict['detection_boxes'] [0:sum(output_dict['detection_scores']>=min_threshold)],\n",
    "              'detection_scores' : output_dict['detection_scores'][0:sum(output_dict['detection_scores']>=min_threshold)]\n",
    "          }\n",
    "            \n",
    "    with open(PATH_TO_BB_HASHMAP, 'w') as f:\n",
    "        for key in bb_hashmap.keys():\n",
    "            f.write(\"%s,%s\\n\"%(key,bb_hashmap[key]))  \n",
    "\n",
    "    return bb_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    \"\"\"This function returns the image into a numpy array of appropriate dimension\"\"\"\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8) if \\\n",
    "        len(np.array(image).shape) == 3 else \\\n",
    "        np.array(image.getdata()).reshape((im_height, im_width, 1)).astype(np.uint8)\n",
    "        \n",
    "def run_inference_for_single_image(image, graph):\n",
    "    \"\"\"This function takes an image and a trained graph and returns an output dictionary with \n",
    "    number of detections made, detection calsses, detection boxes and detection scores. \n",
    "    The user is supposed to filter out the boxes below a minimum threshold\"\"\"\n",
    "    with graph.as_default():\n",
    "      with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "          tensor_name = key + ':0'\n",
    "          if tensor_name in all_tensor_names:\n",
    "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                tensor_name)\n",
    "        if 'detection_masks' in tensor_dict:\n",
    "          # The following processing is only for single image\n",
    "          detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "          detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "          # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "          real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "          detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "          detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "          detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "          detection_masks_reframed = tf.cast(\n",
    "              tf.greater(detection_masks_reframed, 0.1), tf.uint8)\n",
    "          # Follow the convention by adding back the batch dimension\n",
    "          tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "              detection_masks_reframed, 0)\n",
    "        image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "        # Run inference\n",
    "        output_dict = sess.run(tensor_dict,\n",
    "                               feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "        # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "        output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "        output_dict['detection_classes'] = output_dict[\n",
    "            'detection_classes'][0].astype(np.uint8)\n",
    "        output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "        output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "        if 'detection_masks' in output_dict:\n",
    "          output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 0\n",
      "Processing image: 1\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\")\n",
    "import pandas as pd\n",
    "import glob\n",
    "dict1 = {'CaptureEventID':['ASG0002tcc', 'ASG0017xvs'],\n",
    "      'Species': ['giraffe', 'elephant'],\n",
    "      'DateTime': ['2010-09-20', '9/12/2013 18:26'],\n",
    "      'SiteID': ['S0', 'M1'],\n",
    "      'NumSpecies': [1, 1],\n",
    "      'Count': [1, 1],\n",
    "      'urls': ['http://www.snapshotserengeti.org/subjects/standard/50c212718a607540b902010c_0.jpg',\n",
    "               'http://www.snapshotserengeti.org/subjects/standard/5331e2111bccd304b6089b2d_0.JPG'\n",
    "              ]\n",
    "               }\n",
    "df1 = pd.DataFrame(dict1)\n",
    "\n",
    "dataset = df1\n",
    "image_name_index=0\n",
    "url_col_index=6\n",
    "outpath = '/Users/manishrai/Desktop/outdir_test/'\n",
    "\n",
    "get_images_from_url(dataset, image_name_index, url_col_index, outpath)\n",
    "\n",
    "# Model info\n",
    "PATH_TO_FROZEN_GRAPH = '/Users/manishrai/Desktop/UMN/Research/Zooniverse/camera-trap-detection/my_workspace/training_demo/trained-inference-graphs/output_inference_graph/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('/Users/manishrai/Desktop/UMN/Research/Zooniverse/camera-trap-detection/my_workspace/training_demo/annotations/', 'label_map_focus.pbtxt')\n",
    "NUM_CLASSES = 1\n",
    "TEST_IMAGE_PATHS = glob.glob('/Users/manishrai/Desktop/outdir_test/*.jpg')\n",
    "min_threshold = 0.9\n",
    "bb_outpath = '/Users/manishrai/Desktop/test_dir/'\n",
    "PATH_TO_BB_HASHMAP = '/Users/manishrai/Desktop/test_dir/bb_hashmap_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/manishrai/Desktop/outdir_test/ASG0017xvs.jpg',\n",
       " '/Users/manishrai/Desktop/outdir_test/ASG0002tcc.jpg']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASG0017xvs.jpg': {'detection_boxes': array([[0.        , 0.36793044, 1.        , 0.99240124],\n",
       "         [0.        , 0.00238551, 1.        , 0.4733076 ]], dtype=float32),\n",
       "  'detection_scores': array([0.99947053, 0.9561418 ], dtype=float32)},\n",
       " 'ASG0002tcc.jpg': {'detection_boxes': array([[0.30239972, 0.23177734, 0.503748  , 0.40976784]], dtype=float32),\n",
       "  'detection_scores': array([0.9998913], dtype=float32)}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x421 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x421 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_inference_graph_images(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS, \\\n",
    "                               NUM_CLASSES, TEST_IMAGE_PATHS, min_threshold, \\\n",
    "                               bb_outpath, PATH_TO_BB_HASHMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
