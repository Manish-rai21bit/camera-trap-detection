{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject set creation pipeline\n",
    "This notebook demonstrates the pipeline used for creating the subject set. It includes the following codes:<br>\n",
    "- get_images_from_url\n",
    "- run_inference_graph_images\n",
    "- load_image_into_numpy_array\n",
    "- run_inference_for_single_image\n",
    "\n",
    "These functions have to be arranged in modules so that it can be imported for further use. The modules with these functions are available [here](https://github.com/Manish-rai21bit/camera-trap-detection/tree/master/snapshot-safari)\n",
    "\n",
    "This work is a streamlined version of [PredictAndEvaluate](https://github.com/Manish-rai21bit/camera-trap-detection/blob/master/training_demo/PredictAndEvaluation.ipynb)\n",
    "\n",
    "\n",
    "### Running the inferrence detection can be done for bulk images\n",
    "#running inference on the test images <br>\n",
    "#SPLIT=validation  # or test <br>\n",
    "TF\\_RECORD\\_FILES='/home/ubuntu/data/tensorflow/my_workspace/training_demo/annotations/test_schneider.record' <br>\n",
    "#OUTPUT\\_TFR\\_PATH ='/home/ubuntu/data/tensorflow/my_workspace/training_demo/Predictions/test_schneider.record' <br>\n",
    "OUTPUT\\_INFERENCE\\_GRAPH='/home/ubuntu/data/tensorflow/my_workspace/training_demo/trained-inference-graphs/output\\_inference\\_graph/frozen\\_inference\\_graph.pb' <br>\n",
    "\n",
    "python /home/ubuntu/data/tensorflow/models/research/object_detection/inference/infer_detections.py \\ <br>\n",
    "  --input\\_tfrecord_paths=$TF\\_RECORD\\_FILES$ \\ <br>\n",
    "  --output\\_tfrecord\\_path='/home/ubuntu/data/tensorflow/my_workspace/training_demo/Predictions/test_schneider.record'\\ \n",
    "  --inference\\_graph=$OUTPUT\\_INFERENCE\\_GRAPH$ \\ <br>\n",
    "  --discard\\_image\\_pixels<br>\n",
    "\n",
    "\n",
    "#Check the validity of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, time\n",
    "import six.moves.urllib as urllib\n",
    "import sys, glob\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "# Adding path to the utility functions\n",
    "sys.path.append(\"/Users/manishrai/Desktop/UMN/Research/Zooniverse/Code/tensorflow/models/research\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "# Importing the object detection utils\n",
    "# from utils import label_map_util\n",
    "# from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "import os, ssl\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "# from run_inference_graph import load_image_into_numpy_array, run_inference_for_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The set of functions here will help in creating the subject set. \n",
    "Subject set creation for bounding box requires the following functions:\n",
    "    get_images_from_url - Download the images from the url's given into a local path\n",
    "    run_inference_graph_images on the downloaded images\"\"\"\n",
    "\n",
    "def get_images_from_url(dataset, image_name_index, url_col_index, outpath):\n",
    "    \"\"\"This function takes in a dataframe and downloads the images from the url's in the column.\n",
    "    arguments:\n",
    "        dataset - dataframe\n",
    "        image_name_index - index of the column containing the image id (capture event id)\n",
    "        url_col_index - index of the columns containing the url for the image id\n",
    "        outpath - path on the local directory where the image has to be saved\n",
    "    return: \n",
    "        Downloaded images in the path - outpath\n",
    "    Usage:\n",
    "        get_images_from_url(df3, image_name_index=0, url_col_index=6, outpath = '/Users/manishrai/Desktop/test_dir/')\"\"\"\n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "        getattr(ssl, '_create_unverified_context', None)): \n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        \n",
    "        check = []\n",
    "        \n",
    "        for i in range(dataset.shape[0]):\n",
    "            if dataset.iloc[i][image_name_index] not in check:\n",
    "                j = 0\n",
    "            if dataset.iloc[i][image_name_index] in check:\n",
    "                j += 1 \n",
    "            \n",
    "            print('Processing image: %d' % i)\n",
    "            \n",
    "            urllib.request.urlretrieve(dataset.iloc[i][url_col_index], outpath+'{0}.jpg'.format(dataset.iloc[i][image_name_index] ))\n",
    "            \n",
    "def run_inference_graph_images(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS, \\\n",
    "                               NUM_CLASSES, TEST_IMAGE_PATHS, min_threshold, \\\n",
    "                               bb_outpath, PATH_TO_BB_HASHMAP):\n",
    "    \"\"\"This function takes in a list of image local-paths and runs it through the trained graph.\n",
    "    Further, using the visualization function it draws bounding boxes on each of the images and \n",
    "    saves it in a local path.\n",
    "    Arguments:\n",
    "        PATH_TO_FROZEN_GRAPH - local path of the trained frozen graph, '/frozen_inference_graph.pb'\n",
    "        PATH_TO_LABELS - local path of the labels (a mapping from class number to class name), 'label_map_focus.pbtxt'\n",
    "        NUM_CLASSES - number of detection classes\n",
    "        TEST_IMAGE_PATHS - list of test image local paths\n",
    "        min_threshold - minimum score threshold for the bounding box to be considered\n",
    "        bb_outpath - local path where to save the images with ounding poxes, /home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/snapshot-safari/snapshot-serengeti/subject_set_upload/\n",
    "        PATH_TO_BB_HASHMAP - path where bounding box information for the subjects is saved\n",
    "        \n",
    "        \"\"\"\n",
    "    # Loading the frozen graph\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "            \n",
    "    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    \n",
    "    bb_hashmap = {}\n",
    "    for image_path in TEST_IMAGE_PATHS:\n",
    "        image = Image.open(image_path)\n",
    "        if (len(np.array(image).shape) == 3):\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Considering the default dpi of matplotlib, calculating the figure size to save\n",
    "            y0, x0, c = image_np.shape\n",
    "            h = y0/72 # the default for dpi for matplotlib is 72\n",
    "            w = x0/72 # the default for dpi for matplotlib is 72\n",
    "            IMAGE_SIZE = (w, h)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "            # Visualization of the results of a detection.\n",
    "#             vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "#               image_np,\n",
    "#               output_dict['detection_boxes'],\n",
    "#               output_dict['detection_classes'],\n",
    "#               output_dict['detection_scores'],\n",
    "#               category_index,\n",
    "#               instance_masks=output_dict.get('detection_masks'),\n",
    "#               use_normalized_coordinates=True,\n",
    "#               line_thickness=8,\n",
    "#               min_score_thresh=min_threshold,\n",
    "#               skip_labels=True,\n",
    "#               skip_scores=True,\n",
    "#               agnostic_mode=True\n",
    "#             )\n",
    "#             # make a figure without the frame\n",
    "#             fig = plt.figure(frameon=False, figsize=IMAGE_SIZE)\n",
    "#             # make the content fill the whole figure\n",
    "#             ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "#             ax.set_axis_off()\n",
    "#             fig.add_axes(ax)\n",
    "#             # draw your image\n",
    "#             ax.imshow(image_np)\n",
    "#             # saving image with boxes on the disk\n",
    "#             plt.savefig(os.path.join(bb_outpath, '{0}'.format(image_path.split('/')[-1]))) # be careful with the image name \n",
    "#             plt.gcf().clear()\n",
    "            bb_hashmap[image_path.split('/')[-1]] = {\n",
    "              'detection_boxes' : output_dict['detection_boxes'][0:sum(output_dict['detection_scores']>=min_threshold)],\n",
    "              'detection_scores' : output_dict['detection_scores'][0:sum(output_dict['detection_scores']>=min_threshold)],\n",
    "              'detection_classes' : output_dict['detection_classes'][0:sum(output_dict['detection_scores']>=min_threshold)]\n",
    "#               'num_detections' : output_dict['num_detections']\n",
    "          }\n",
    "            \n",
    "    with open(PATH_TO_BB_HASHMAP, 'w') as f:\n",
    "        for key in bb_hashmap.keys():\n",
    "            f.write(\"%s,%s\\n\"%(key,bb_hashmap[key]))  \n",
    "\n",
    "    return bb_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    \"\"\"This function returns the image into a numpy array of appropriate dimension\"\"\"\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8) if \\\n",
    "        len(np.array(image).shape) == 3 else \\\n",
    "        np.array(image.getdata()).reshape((im_height, im_width, 1)).astype(np.uint8)\n",
    "        \n",
    "def run_inference_for_single_image(image, graph):\n",
    "    \"\"\"This function takes an image and a trained graph and returns an output dictionary with \n",
    "    number of detections made, detection calsses, detection boxes and detection scores. \n",
    "    The user is supposed to filter out the boxes below a minimum threshold\"\"\"\n",
    "    with graph.as_default():\n",
    "      with tf.Session() as sess:\n",
    "        # Get handles to input and output tensors\n",
    "        ops = tf.get_default_graph().get_operations()\n",
    "        all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "        tensor_dict = {}\n",
    "        for key in [\n",
    "            'num_detections', 'detection_boxes', 'detection_scores',\n",
    "            'detection_classes', 'detection_masks'\n",
    "        ]:\n",
    "          tensor_name = key + ':0'\n",
    "          if tensor_name in all_tensor_names:\n",
    "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                tensor_name)\n",
    "        if 'detection_masks' in tensor_dict:\n",
    "          # The following processing is only for single image\n",
    "          detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "          detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "          # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "          real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "          detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "          detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "          detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "          detection_masks_reframed = tf.cast(\n",
    "              tf.greater(detection_masks_reframed, 0.1), tf.uint8)\n",
    "          # Follow the convention by adding back the batch dimension\n",
    "          tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "              detection_masks_reframed, 0)\n",
    "        image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "        # Run inference\n",
    "        output_dict = sess.run(tensor_dict,\n",
    "                               feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "        # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "        output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "        output_dict['detection_classes'] = output_dict[\n",
    "            'detection_classes'][0].astype(np.uint8)\n",
    "        output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "        output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "        if 'detection_masks' in output_dict:\n",
    "          output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Varaible assignment:** For running the inference graph we need to define the variables below\n",
    "\n",
    "History of runs: <br>\n",
    "    **1. APNR**<br>\n",
    "PATH_TO_LABELS = os.path.join('/home/ubuntu/data/tensorflow/my_workspace/training_demo/annotations/', 'label_map_focus.pbtxt')<br>\n",
    "TEST_IMAGE_PATHS = glob.glob('/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/snapshot-safari/APNR/subject_set/*.JPG')<br>\n",
    "min_threshold = 0.9 <br>\n",
    "NUM_CLASSES = 1 <br>\n",
    "bb_outpath = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/snapshot-safari/APNR/subject_set_upload/' <br>\n",
    "PATH_TO_BB_HASHMAP = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/snapshot-safari/APNR/bb_hashmap_APNR.csv'\n",
    "\n",
    "**2. Schneider's data**<br>\n",
    "PATH_TO_FROZEN_GRAPH = '/home/ubuntu/data/tensorflow/my_workspace/training_demo/trained-inference-graphs/output_inference_graph/frozen_inference_graph.pb' <br>\n",
    "PATH_TO_LABELS = os.path.join('/home/ubuntu/data/tensorflow/my_workspace/training_demo/annotations/', 'label_map_lila.pbtxt') <br>\n",
    "TEST_IMAGE_PATHS = glob.glob('/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_images/*.jpg') <br>\n",
    "min_threshold = 0.9 <br>\n",
    "NUM_CLASSES = 49 <br>\n",
    "bb_outpath = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_bb_prediction/' <br>\n",
    "PATH_TO_BB_HASHMAP = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_bb_prediction/bb_hashmap_schneider.csv' <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run for APNR\n",
    "PATH_TO_FROZEN_GRAPH = '/home/ubuntu/data/tensorflow/my_workspace/training_demo/trained-inference-graphs/output_inference_graph/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('/home/ubuntu/data/tensorflow/my_workspace/training_demo/annotations/', 'label_map_lila.pbtxt')\n",
    "TEST_IMAGE_PATHS = glob.glob('/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_images/*.jpg')\n",
    "min_threshold = 0.9\n",
    "NUM_CLASSES = 49\n",
    "bb_outpath = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_bb_prediction/'\n",
    "PATH_TO_BB_HASHMAP = '/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_bb_prediction/bb_hashmap_schneider.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASG000e8t5.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGE_PATHS[8].split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the inference graph** \n",
    "- Run inference graph on test dataset\n",
    "- Save images with bounding boxes in the output directory\n",
    "- Save the dictionaty with bounding box and score to a location on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "run_inference_graph_images(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS, \\\n",
    "                               NUM_CLASSES, TEST_IMAGE_PATHS, min_threshold, \\\n",
    "                               bb_outpath, PATH_TO_BB_HASHMAP)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 0\n",
      "Processing image: 1\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\")\n",
    "import pandas as pd\n",
    "import glob\n",
    "dict1 = {'CaptureEventID':['ASG0002tcc', 'ASG0017xvs'],\n",
    "      'Species': ['giraffe', 'elephant'],\n",
    "      'DateTime': ['2010-09-20', '9/12/2013 18:26'],\n",
    "      'SiteID': ['S0', 'M1'],\n",
    "      'NumSpecies': [1, 1],\n",
    "      'Count': [1, 1],\n",
    "      'urls': ['http://www.snapshotserengeti.org/subjects/standard/50c212718a607540b902010c_0.jpg',\n",
    "               'http://www.snapshotserengeti.org/subjects/standard/5331e2111bccd304b6089b2d_0.JPG'\n",
    "              ]\n",
    "               }\n",
    "df1 = pd.DataFrame(dict1)\n",
    "\n",
    "dataset = df1\n",
    "image_name_index=0\n",
    "url_col_index=6\n",
    "outpath = '/Users/manishrai/Desktop/outdir_test/'\n",
    "\n",
    "get_images_from_url(dataset, image_name_index, url_col_index, outpath)\n",
    "\n",
    "# Model info\n",
    "PATH_TO_FROZEN_GRAPH = '/Users/manishrai/Desktop/UMN/Research/Zooniverse/camera-trap-detection/my_workspace/training_demo/trained-inference-graphs/output_inference_graph/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('/Users/manishrai/Desktop/UMN/Research/Zooniverse/camera-trap-detection/my_workspace/training_demo/annotations/', 'label_map_focus.pbtxt')\n",
    "NUM_CLASSES = 1\n",
    "TEST_IMAGE_PATHS = glob.glob('/Users/manishrai/Desktop/outdir_test/*.jpg')\n",
    "min_threshold = 0.9\n",
    "bb_outpath = '/Users/manishrai/Desktop/test_dir/'\n",
    "PATH_TO_BB_HASHMAP = '/Users/manishrai/Desktop/test_dir/bb_hashmap_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/data/tensorflow/my_workspace/camera-trap-detection/data/LILA/schneider_images/ASG000blvv.jpg']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGE_PATHS = TEST_IMAGE_PATHS[0:1]\n",
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.630385637283325 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Comparing runtime with and without disk write for boundin box images\n",
    "import time\n",
    "start_time = time.time()\n",
    "run_inference_graph_images(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS, \\\n",
    "                               NUM_CLASSES, TEST_IMAGE_PATHS, min_threshold, \\\n",
    "                               bb_outpath, PATH_TO_BB_HASHMAP)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with disk write - 31.211701154708862 seconds\n",
    "without disk write of images - 28.77697730064392 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing for speedup of the running the inference graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    \"\"\"This function returns the image into a numpy array of appropriate dimension\"\"\"\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8) if \\\n",
    "        len(np.array(image).shape) == 3 else \\\n",
    "        np.array(image.getdata()).reshape((im_height, im_width, 1)).astype(np.uint8)\n",
    "        \n",
    "def run_inference_for_images(image_dir, PATH_TO_FROZEN_GRAPH):\n",
    "    \"\"\"This function takes an image and a trained graph and returns an output dictionary with \n",
    "    number of detections made, detection calsses, detection boxes and detection scores. \n",
    "    The user is supposed to filter out the boxes below a minimum threshold\"\"\"\n",
    "    \n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "      od_graph_def = tf.GraphDef()\n",
    "      with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "    with detection_graph.as_default():\n",
    "      with tf.Session() as sess:\n",
    "        output_dict_array = []\n",
    "        bb_hashmap = {}\n",
    "        for image_path in image_dir:\n",
    "        # Get handles to input and output tensors\n",
    "            image = Image.open(image_path)\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            \n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "              tensor_name = key + ':0'\n",
    "              if tensor_name in all_tensor_names:\n",
    "                tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                    tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "              # The following processing is only for single image\n",
    "              detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "              detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "              # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "              real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "              detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "              detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "              detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                  detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "              detection_masks_reframed = tf.cast(\n",
    "                  tf.greater(detection_masks_reframed, 0.1), tf.uint8)\n",
    "              # Follow the convention by adding back the batch dimension\n",
    "              tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                  detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                   feed_dict={image_tensor: np.expand_dims(image_np, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "#             if 'detection_masks' in output_dict:\n",
    "#               output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "            \n",
    "            bb_hashmap[image_path.split('/')[-1]] = {\n",
    "            'detection_boxes' : output_dict['detection_boxes'][0:sum(output_dict['detection_scores']>=min_threshold)],\n",
    "            'detection_scores' : output_dict['detection_scores'][0:sum(output_dict['detection_scores']>=min_threshold)],\n",
    "            'detection_classes' : output_dict['detection_classes'][0:sum(output_dict['detection_scores']>=min_threshold)]\n",
    "            }\n",
    "            output_dict_array.append(output_dict)\n",
    "            \n",
    "    return output_dict_array, bb_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.049812316894531 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output_dict_array, bb_hashmap = run_inference_for_images(TEST_IMAGE_PATHS, PATH_TO_FROZEN_GRAPH)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.88"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((28.77697730064392 - 17.27455163002014)/4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASG000bcnc.jpg': {'detection_boxes': array([[0.5546642 , 0.78439456, 0.79757255, 0.9995391 ]], dtype=float32),\n",
       "  'detection_classes': array([5], dtype=uint8),\n",
       "  'detection_scores': array([0.9459807], dtype=float32)},\n",
       " 'ASG000blvv.jpg': {'detection_boxes': array([[0.36486182, 0.24578457, 0.6627488 , 0.63912284],\n",
       "         [0.43956482, 0.6381016 , 0.5821619 , 0.87734836],\n",
       "         [0.5026478 , 0.8996175 , 0.54911417, 0.93767565],\n",
       "         [0.4248089 , 0.9507573 , 0.5720013 , 0.99983895]], dtype=float32),\n",
       "  'detection_classes': array([17, 17, 11, 17], dtype=uint8),\n",
       "  'detection_scores': array([0.9947514 , 0.98929214, 0.92486733, 0.92416286], dtype=float32)},\n",
       " 'ASG000cewa.jpg': {'detection_boxes': array([[0.00149002, 0.08484562, 1.        , 0.97310466]], dtype=float32),\n",
       "  'detection_classes': array([5], dtype=uint8),\n",
       "  'detection_scores': array([0.99928856], dtype=float32)},\n",
       " 'ASG000cq2q.jpg': {'detection_boxes': array([[0.        , 0.        , 0.94640434, 0.9904304 ]], dtype=float32),\n",
       "  'detection_classes': array([15], dtype=uint8),\n",
       "  'detection_scores': array([0.9930496], dtype=float32)}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.318637907505035"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17.27455163002014/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3753127574920656"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67.50625514984131/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2212302148342133"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128.84920859336853/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1367027908563614"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "250.9362232685089/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1181362926959992"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "374.1763551235199/120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.090312175452709"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "494.4499480724335/160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Time per image (seconds)')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([1, 4, 20, 40, 80, 120, 160], \\\n",
    "         [8.049812316894531, \\\n",
    "          4.318637907505035, \\\n",
    "          3.3753127574920656, \\\n",
    "          3.2212302148342133, \\\n",
    "          3.1367027908563614, \\\n",
    "          3.1181362926959992, \\\n",
    "          3.090312175452709], '-')\n",
    "plt.grid()\n",
    "plt.title('Time per batch-size')\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('Time per image (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
